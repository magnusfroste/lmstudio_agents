# LM Studio Function Calling Example

This project demonstrates how to use LM Studio's function calling capability to enable an LLM (like Qwen) to multiply two numbers using a Python function.

## Project Structure

This project provides a modular framework for interacting with an LLM using function calling in LM Studio. The structure is designed to be easily extensible for adding new tools:

- **`llmchat.py`**: The main script that handles chat interactions with the LLM and routes tool calls to the appropriate functions.
- **`tools/`**: A directory containing modules for each tool/function:
  - **`math_operations.py`**: Contains functions for mathematical operations like multiplication.
  - **`web_requests.py`**: Contains functions for making HTTP requests.
  - **`database_operations.py`**: Contains functions for querying a SQLite database of product sales.
- **`create_sales_database.py`**: A utility script to generate a sample SQLite database with product sales data.
- **`product_sales.db`**: The SQLite database file containing sample sales data (generated by running `create_sales_database.py`).

## Prerequisites

- **LM Studio**: Ensure LM Studio is installed and running as a server with a model like Qwen2.5-7B-Instruct-GGUF loaded.
- **Python**: Version 3.6 or higher.
- **OpenAI Library**: For interacting with LM Studio's API.
- **Requests Library**: For making HTTP requests.

## Setup

### Installation

1. **Clone the Repository**: Clone this repository to your local machine.
2. **Run the Installation Script**: Navigate to the project directory and run `./install.sh`. This script now offers an **interactive menu** that will guide you through the setup process with step-by-step feedback and confirmation prompts. It will:
   - Check if Python 3.6 or higher is installed.
   - Create and activate a virtual environment if not already activated.
   - Install the required dependencies from `requirements.txt`.
   - Check if the LM Studio server is running and offer to start it if not.
   - Create the sample sales database if it doesn't exist.
3. **Start LM Studio Server**: If the server is not running, the script will prompt you to start it with `lms server start` and ensure a model like Qwen2.5-7B-Instruct-GGUF is loaded.

**Note**: If you encounter issues with the server not being detected, ensure `lms` is installed and accessible in your terminal. You can manually start the server with `lms server start` before running the script again. If the virtual environment is not activated automatically, you can activate it manually with `source venv/bin/activate`.

## Running the Project

Run the main script to start interacting with the LLM:

```bash
python llmchat.py
```

**Interface Note**: The interaction with the LLM occurs through a terminal window (command line interface). After running the script, a chat-like interface will open in your terminal where you can type your messages to communicate with the LLM. This is not a web-based chat; it operates entirely within the terminal. Type your messages and press Enter to send them, and type `exit` to stop the interaction.

Type your messages to chat with the LLM, and type `exit` to stop.

**Note**: Before querying sales data, ensure the sample database has been created. The installation script now handles this automatically, but if you need to regenerate it, run `python create_sales_database.py` manually.

**Asking About Sales Data**: You can ask the LLM about sales data using natural language questions like 'How much was sold in May 2025?' or 'What were the sales for last month?'. The LLM will interpret your request and retrieve the total sales revenue and number of items sold for the specified month. Note that the sample data covers approximately the last three months from the date the database was created, so queries for future dates or much older dates may return no results.

**Advanced Sales Queries**: Beyond monthly totals, you can also ask for:
- A list of all unique products sold with quantities and total revenue (e.g., 'List all sold products' or 'Show me the sales inventory').
- The top most expensive product sales with details like price and sale date (e.g., 'What are the 5 most expensive products sold?' or 'Show top 3 highest priced sales').

**Discovering Available Functions**: If you're unsure about what I can do, simply ask 'What functions do you support?' or 'List available tools'. I will provide a list of all the tools and functions I can use to assist you, along with a brief description of each.

## Extending the Project

To add more functions/tools for the LLM to use, follow these steps:

1. **Create a New Tool Module**: Add a new Python file in the `tools/` directory for your function (e.g., `tools/new_tool.py`).
2. **Define the Function**: Write your function with appropriate docstrings and type hints. For example:
   ```python
   def new_function(param1: str) -> str:
       """Description of what the function does."""
       # Function implementation
       return result
   ```
3. **Add the Tool to the Tools List**: Update the `tools` list in `llmchat.py` to include the new function:
   ```python
   tools = [
       # Existing tools
       {
           "type": "function",
           "function": {
               "name": "new_function",
               "description": "Description of what the function does.",
               "parameters": {
                   "type": "object",
                   "properties": {
                       "param1": {
                           "type": "string",
                           "description": "Description of param1"
                       }
                   },
                   "required": ["param1"]
               }
           }
       }
   ]
   ```
4. **Update the Router**: Modify the `handle_tool_call` function in `llmchat.py` to handle the new tool:
   ```python
   elif tool_name == "new_function":
       result = new_function(tool_arguments['param1'])
       return f"Result of new_function: {result}"
   ```
5. **Import the Function**: Ensure the new function is imported at the top of `llmchat.py`:
   ```python
   from tools.new_tool import new_function
   ```

This modular structure allows you to keep adding new tools without cluttering the main script, maintaining a clean and scalable codebase.

**Example Additional Tool**: A tool for querying sales data by month has been successfully added. You can ask the LLM questions like "How much was sold in March 2025?" by specifying the month in 'YYYY-MM' format (e.g., '2025-03'). The tool will return the total sales revenue and the number of items sold for that month, based on the sample data in the database.

Additionally, tools for more flexible database interaction have been implemented:
- `list_all_sold_products`: Returns a list of all unique products sold, showing how many units of each were sold and the total revenue generated.
- `get_top_expensive_products`: Retrieves the top most expensive individual sales (defaulting to top 5), including product name, price, and sale date.
- `list_available_tools`: Allows users to see all supported functions and capabilities of the LLM. Simply ask 'What functions do you support?' or 'List available tools' to get a comprehensive list of what I can do to assist you.

## Extending with More Functions (Tool Calling)

You can extend this project by adding more functions that the LLM can call as tools. Here's how to add a new tool for making an HTTP request:

1. **Define the New Function**: Add a new function in `web_requests.py` to handle HTTP requests. For example:
   ```python
   import requests

   def make_http_request(url: str) -> str:
       """Make an HTTP GET request to the specified URL and return the response text."""
       try:
           response = requests.get(url)
           response.raise_for_status()
           print(f"HTTP request to {url} successful")
           return response.text[:500] + "..." if len(response.text) > 500 else response.text
       except Exception as e:
           print(f"Error making HTTP request to {url}: {str(e)}")
           return f"Error: {str(e)}"
   ```

2. **Add the Tool to the Tools List**: Update the `tools` list to include the new function:
   ```python
   tools = [
       {
           "type": "function",
           "function": {
               "name": "multiply_numbers",
               "description": "Multiply two numbers and return the result.",
               "parameters": {
                   "type": "object",
                   "properties": {
                       "a": {
                           "type": "number",
                           "description": "The first number"
                       },
                       "b": {
                           "type": "number",
                           "description": "The second number"
                       }
                   },
                   "required": ["a", "b"]
               }
           }
       },
       {
           "type": "function",
           "function": {
               "name": "make_http_request",
               "description": "Make an HTTP GET request to a specified URL and return the response.",
               "parameters": {
                   "type": "object",
                   "properties": {
                       "url": {
                           "type": "string",
                           "description": "The URL to make the HTTP GET request to"
                       }
                   },
                   "required": ["url"]
               }
           }
       }
   ]
   ```

3. **Update the Chat Loop to Handle the New Tool**: Modify the `main()` function to process calls to the new tool:
   ```python
   # Check if the assistant wants to call a tool
   if assistant_message.tool_calls:
       for tool_call in assistant_message.tool_calls:
           if tool_call.function == "multiply_numbers":
               args = json.loads(tool_call.arguments)
               result = multiply_numbers(args["a"], args["b"])
               messages.append({
                   "role": "tool",
                   "content": str(result),
                   "tool_call_id": tool_call.id
               })
               # Get the final response from the model after tool execution
               final_message = chat_with_model(messages)
               messages.append(final_message)
               print(f"Assistant (after calculation): {final_message.content}")
           elif tool_call.function == "make_http_request":
               args = json.loads(tool_call.arguments)
               result = make_http_request(args["url"])
               messages.append({
                   "role": "tool",
                   "content": result,
                   "tool_call_id": tool_call.id
               })
               # Get the final response from the model after tool execution
               final_message = chat_with_model(messages)
               messages.append(final_message)
               print(f"Assistant (after HTTP request): {final_message.content}")
   ```

4. **Install Additional Dependencies**: If your new function requires additional libraries (like `requests` for HTTP requests), install them in your virtual environment:
   ```bash
   source venv/bin/activate  # On macOS/Linux
   # OR
   venv\Scripts\activate  # On Windows
   pip install requests
   ```

5. **Update System Message**: Optionally, update the system message to inform the model about the new capability:
   ```python
   messages = [
       {"role": "system", "content": "You are a helpful assistant that can use tools to answer questions. If you don't know the answer, you can search for information. Always use the provided tools to assist with the queries. If a tool is not available for a specific task, inform the user and suggest an alternative approach."}
   ]
   ```

Now, when interacting with the model, you can ask it to fetch content from a URL, for example: "Can you get information from https://api.example.com/data?" and it will call the `make_http_request` function to retrieve the data.

Remember to handle errors appropriately in your functions and limit response sizes if necessary to avoid overwhelming the model with too much data.

## System Prompt

The system prompt used to initialize the chat with the LLM is as follows:

```plaintext
You are a helpful assistant that can use tools to answer questions. If you don't know the answer, you can search for information. Always use the provided tools to assist with the queries. If a tool is not available for a specific task, inform the user and suggest an alternative approach.
```

This prompt ensures that the model leverages the available tools to provide accurate and helpful responses.
